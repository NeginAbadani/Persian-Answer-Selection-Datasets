<span align="center">
    <a href="https://www.kaggle.com/datasets/neginabadani/persian-answer-selection-datasets"><img alt="Kaggle" src="https://img.shields.io/static/v1?label=Kaggle&message=Persian Answer Selection Datasets&logo=Kaggle&color=20BEFF"/></a>
</span>

# Persian Answer Selection Datasets
Recent developments in Answer Selection (QA) have improved state-of-the-art results, and various datasets have been released for this task. Due to the lack of Persian factoid open-domain datasets, less research has been done on the latter language, making comparisons difficult. We present four Persian Answer Selection datasets, three of which have been generated by translating three benchmark datasets ([`WikiQA`](www.microsoft.com/en-us/download/details.aspx?id=52419), [`TrecQA Raw, TrecQA Clean`](cs.stanford.edu/people/mengqiu/data/qg-emnlp07-data.tgz)) and one has been generated using a native Question Answering dataset for Persian QA, namely [`PersianQuAD`](https://github.com/BigData-IsfahanUni/PersianQuAD/tree/main). In addition, we have trained two baseline models, i.e., BERT and RoBERTa, on each dataset. 
# Dataset
### Download
All four datasets are available for download from the [`Datasets`](https://github.com/NeginAbadani/Persian-Answer-Selection-Datasets/tree/main/Datasets) directory. The statistics of the four datasets are shown below:
| Dataset      | Split | No. of questions | No. of candidate answers | Average No. of candidate answers |
|--------------|-------|------------------|--------------------------|----------------------------------|
| TrecQA-Raw   | Train | 1229             | 53417                    | 43.46                            |
| TrecQA-Raw   | Dev   | 81               | 1148                     | 14.97                            |
| TrecQA-Raw   | Test  | 95               | 1517                     | 15.97                            |
| TrecQA-Clean | Train | 1229             | 53147                    | 43.46                            |
| TrecQA-Clean | Dev   | 65               | 1117                     | 17.18                            |
| TrecQA-Clean | Test  | 68               | 1442                     | 21.21                            |
| WikiQA       | Train | 2118             | 20360                    | 9.61                             |
| WikiQA       | Dev   | 296              | 2733                     | 9.23                             |
| WikiQA       | Test  | 633              | 6165                     | 9.74                             |
| PersianQuAD  | Train | 14078            | 110321                   | 7.84                             |
| PersianQuAD  | Dev   | 3499             | 27759                    | 7.93                             |
| PersianQuAD  | Test  | 996              | 7753                     | 7.78                             |
# Evalution
In order to evaluate the quality of our dataset and compare it with the benchmark English datatsets, two QA models have been trained. The pre-trained BERT (ParsBERT for the Persian datasets) and RoBERTa models have been fine-tuned on all datasets. We evaluate each model using two widely used automatic evaluation metrics *MAP* and *MRR*.

| Dataset                     | Model    | MAP       | MRR       |
|-----------------------------|----------|-----------|-----------|
| TrecQA-Raw                  | BERT     | 0.913     | 0.960     |
| TrecQA-Raw                  | RoBERTa  | 0.927     | 0.970     |
| **TrecQA-Raw-Translated**   | **BERT** | **0.855** | **0.921** |
| TrecQA-Raw-Translated       | RoBERTa  | 0.829     | 0.887     |
| TrecQA-Clean                | BERT     | 0.886     | 0.947     |
| TrecQA-Clean                | RoBERTa  | 0.905     | 0.961     |
| **TrecQA-Clean-Translated** | **BERT** | **0.815** | **0.895** |
| TrecQA-Clean-Translated     | RoBERTa  | 0.774     | 0.854     |
| WikiQA                      | BERT     | 0.797     | 0.809     |
| WikiQA                      | RoBERTa  | 0.811     | 0.823     |
| **WikiQA-Translated**       | **BERT** | **0.729** | **0.744** |
| WikiQA-Translated           | RoBERTa  | 0.670     | 0.686     |
| **PersianQuAD**             | **BERT** | **0.846** | **0.867** |
| PersianQuAD                 | RoBERTa  | 0.805     | 0.827     |

# Citation
### Plain
Will be added soon...

### Bibtex
Will be added soon...
